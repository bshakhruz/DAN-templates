{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "uFSCmiR4mAoN",
        "MreWcG6_ol7x"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bshakhruz/DAN-templates/blob/main/DNN_video_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Deep Neural Networks on Videos Using Google Colab\n",
        "---\n"
      ],
      "metadata": {
        "id": "vw7kfwB_Ond4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction to Google Colab\n",
        "\n",
        "Google Colab is a free cloud service based on Jupyter Notebooks that supports free GPU and TPU usage. It's ideal for machine learning, data analysis, and education. The platform eliminates the need for expensive hardware, making deep learning more accessible.\n",
        "\n"
      ],
      "metadata": {
        "id": "W1X96z7mRv8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setting up the Colab Environment\n",
        "\n",
        "- **Enable GPU/TPU**: Go to `Edit` > `Notebook Settings` or `Runtime` > `Change runtime type` and select GPU or TPU from the dropdown to accelerate your computations.\n",
        "\n",
        "**Checkpoint:** Remember to save your notebook now to preserve this setting.\n",
        "\n",
        "## Verifying the Accelerator Status\n",
        "\n",
        "After enabling the hardware accelerator, run the following cell to confirm that it is active:\n"
      ],
      "metadata": {
        "id": "kuUXcEX2Qp67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the hardware accelerator (GPU/TPU)\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print('No GPU found')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "8hq3eWVtq5Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Installing Dependencies\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install tensorflow opencv-python-headless\n",
        "!pip3 install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "DIztlvGSSF0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify installation\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import torch\n",
        "print(f'TensorFlow version: {tf.__version__}')\n",
        "print(f'OpenCV version: {cv2.__version__}')\n",
        "print(f'PyTorch version: {torch.__version__}')"
      ],
      "metadata": {
        "id": "dpmCICN0rJ2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Accessing and Preparing Video Data\n",
        "\n",
        "Training deep learning models on video data requires organizing and preprocessing your video files into a format that your model can process. This section guides you through accessing your video data, whether stored locally or on Google Drive, and details the preprocessing steps necessary to prepare the data for model training.\n",
        "\n",
        "## Preparing the Environment\n",
        "\n",
        "Before you start preprocessing your video data, you'll need to install some additional dependencies.\n"
      ],
      "metadata": {
        "id": "1wenL1i1UIEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Additional Dependencies for Preprocessing Phase\n",
        "!apt update && !apt install ffmpeg\n",
        "!pip install moviepy"
      ],
      "metadata": {
        "id": "mnxbVB1-SOnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive (Optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i8jHmSoVUREH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MTTQ0MaNF-Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O /content/file.zip \"https://drive.google.com/uc?export=download&id=YOUR_FILE_ID\""
      ],
      "metadata": {
        "id": "Qkn7OkBSF_1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/file.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ],
      "metadata": {
        "id": "ckn3CBPjGMG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This snippet loads a video, extract frames, and preprocess them,\n",
        "# NOTE: place your actual path in the 'video_path' variable\n",
        "\n",
        "# Necessary library imports\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Placeholder for directory where videos are located\n",
        "video_directory = '<path_to_videos>' # :example: /content/\n",
        "\n",
        "# List all video files in the directory\n",
        "video_files = [file for file in os.listdir(video_directory) if file.endswith('.mp4')]\n",
        "\n",
        "# Placeholder for labeling videos (replace with your own labels)\n",
        "video_labels = {\n",
        "    \"example.mp4\": 0,\n",
        "    'example1.mp4': 1,\n",
        "    # Add as many videos as you have...\n",
        "}\n",
        "# Function to extract and preprocess frames for a batch of videos\n",
        "def extract_and_preprocess_batch(video_paths, labels, skip_frames=5, batch_size=32):\n",
        "    frames_batch = []\n",
        "    labels_batch = []\n",
        "    for video_path, label in zip(video_paths, labels):\n",
        "        count = 0\n",
        "        frames = []\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            if count % skip_frames == 0:\n",
        "                # Preprocess steps (e.g., resizing, normalization)\n",
        "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                resized_frame = cv2.resize(frame_rgb, (112, 112))  # Resize frame to model input size\n",
        "                frames.append(resized_frame / 255.0)  # Normalize pixel values\n",
        "            count += 1\n",
        "\n",
        "        cap.release()\n",
        "        frames_batch.extend(frames)\n",
        "        labels_batch.extend([label] * len(frames))\n",
        "        if len(frames_batch) >= batch_size:\n",
        "            yield np.array(frames_batch), np.array(labels_batch)\n",
        "            frames_batch = []\n",
        "            labels_batch = []\n",
        "\n",
        "    if frames_batch:\n",
        "        yield np.array(frames_batch), np.array(labels_batch)\n",
        "\n",
        "# Process videos in batches\n",
        "batch_size = 32\n",
        "processed_frames_batches = []\n",
        "labels_batches = []\n",
        "\n",
        "for batch_frames, batch_labels in extract_and_preprocess_batch(video_files, video_labels.values(), batch_size=batch_size):\n",
        "    processed_frames_batches.append(batch_frames)\n",
        "    labels_batches.append(batch_labels)\n",
        "    print(f\"Processed batch with {len(batch_frames)} frames\")\n",
        "\n",
        "# Concatenate batches\n",
        "processed_frames = np.concatenate(processed_frames_batches, axis=0)\n",
        "labels = np.concatenate(labels_batches, axis=0)\n",
        "\n",
        "# After processing all batches\n",
        "print(f\"Total videos processed: {len(processed_frames_batches) * batch_size}\")\n",
        "print(f\"Total frames: {len(processed_frames)}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "# Optionally, display a frame to verify preprocessing\n",
        "if len(processed_frames) > 0:\n",
        "    plt.imshow(processed_frames[0])\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No frames available to display.\")"
      ],
      "metadata": {
        "id": "FnsfWYTtVGON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Organizing the Dataset\n",
        "\n",
        "After preprocessing the video data, the next step is to organize the dataset into distinct sets for training, validation, and testing. This ensures that we can train our model, fine-tune hyperparameters, and evaluate performance effectively.\n",
        "\n",
        "## Splitting the Dataset\n",
        "\n",
        "We will use `train_test_split` from `sklearn` to partition the data. The `test_size` parameter determines the proportion of the dataset to include in the test split."
      ],
      "metadata": {
        "id": "-fLvTwYivbIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'processed_frames' contains all your preprocessed video frames\n",
        "# and 'labels' is an array of corresponding labels for each video.\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    processed_frames, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Further split the training set to create a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.25, random_state=42\n",
        ")  # 0.25 x 0.8 = 0.2 of the original dataset # Adjust test_size as per your requirement\n",
        "\n",
        "\n",
        "# Note: Adjust the test_size parameter based on how much data you want to allocate for testing and validation.\n",
        "# Now X_train, X_val, and X_test along with y_train, y_val, and y_test are ready to be used in the model training process.\n",
        "\n",
        "# Save arrays to .npz file\n",
        "np.savez('/content/dataset_splits.npz', X_train=X_train, X_val=X_val, X_test=X_test, y_train=y_train, y_val=y_val, y_test=y_test)"
      ],
      "metadata": {
        "id": "U3QeNaW8h5_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Data Splits\n",
        "import numpy as np\n",
        "\n",
        "# Replace '/content/dataset_splits.npz' with your preferred save path\n",
        "np.savez(\n",
        "    '/content/dataset_splits.npz',\n",
        "    X_train=X_train, X_val=X_val, X_test=X_test,\n",
        "    y_train=y_train, y_val=y_val, y_test=y_test\n",
        ")"
      ],
      "metadata": {
        "id": "D-SNmEnCwej_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Data Augmentation (Optional)\n",
        "\n",
        "Data augmentation is an essential technique in the machine learning workflow, particularly when dealing with image and video data. By applying random transformations to your training data, you can artificially expand the size and variance of your dataset. This process is key to preventing overfitting and helps the model generalize better to new, unseen data.\n",
        "\n",
        "In this section, we'll use Keras's preprocessing layers to implement on-the-fly data augmentation.\n",
        "\n",
        "## Why Data Augmentation?\n",
        "\n",
        "- **Improves Generalization:** By simulating a broader set of variations, the model is less likely to memorize specific data points.\n",
        "- **Addresses Overfitting:** Especially in scenarios with limited data, augmentation can effectively increase the dataset size.\n",
        "- **Enhances Robustness:** Models trained with augmented data often perform better in real-world scenarios where data imperfections are common.\n",
        "\n",
        "## Implementing Data Augmentation\n",
        "\n",
        "The `ImageDataGenerator` class in Keras provides a suite of tools for on-the-fly image augmentation."
      ],
      "metadata": {
        "id": "6gd6OmF4L2RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define your data augmentation pipeline\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,        # Random rotations from 0 to 40 degrees\n",
        "    width_shift_range=0.2,    # Random horizontal shifts\n",
        "    height_shift_range=0.2,   # Random vertical shifts\n",
        "    shear_range=0.2,          # Shear transformations\n",
        "    zoom_range=0.2,           # Random zoom\n",
        "    horizontal_flip=True,     # Random horizontal flips\n",
        "    fill_mode='nearest'       # Strategy for filling in new pixels\n",
        ")\n",
        "\n",
        "# Visualization of Data Augmentation\n",
        "# Let's visualize some augmented examples to ensure our transformations are correct.\n",
        "x_sample = X_train[0]\n",
        "y_sample = y_train[0]\n",
        "\n",
        "# Generate and plot a batch of augmented images\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "axes = axes.flatten()\n",
        "for ax in axes:\n",
        "    # Apply a random transformation\n",
        "    augmented_image = datagen.random_transform(x_sample)\n",
        "    ax.imshow(augmented_image)\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# To use data augmentation during training, pass the datagen.flow(...) as the training data in model.fit\n",
        "# Example: model.fit(datagen.flow(X_train, y_train, batch_size=32), ...)"
      ],
      "metadata": {
        "id": "qWkw-w4fMFlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Overview of Different Architectures for Video Analysis\n",
        "\n",
        "Selecting the right architecture for video analysis is pivotal to the success of your machine learning project. Below, we delve into several popular architectures, highlighting their uses and explaining how they work. Understanding these will help you choose the best fit for your project's needs.\n",
        "\n",
        "## CNNs (Convolutional Neural Networks)\n",
        "\n",
        "- **Use:** Primarily for extracting spatial features from video frames.\n",
        "- **Explanation:** CNNs excel in identifying patterns, shapes, and textures within images, making them suitable for frame-level analysis.\n",
        "- **Foundational Papers:** [Gradient-based learning applied to document recognition by LeCun et al.](https://ieeexplore.ieee.org/document/726791)\n",
        "\n",
        "## RNN/LSTM/GRU\n",
        "\n",
        "- **Use:** Best for analyzing temporal dependencies in video sequences.\n",
        "- **Explanation:** RNNs and their variants, LSTM and GRU, are designed to model sequential data, capturing the temporal dynamics crucial for understanding video content over time.\n",
        "- **Foundational Papers:**\n",
        "  - RNN: [Finding Structure in Time by Elman.](https://crl.ucsd.edu/~elman/Papers/fsit.pdf)\n",
        "  - LSTM: [Long Short-Term Memory by Hochreiter & Schmidhuber.](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
        "  - GRU: [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation by Cho et al.](https://arxiv.org/abs/1406.1078)\n",
        "\n",
        "## 3D CNNs\n",
        "\n",
        "- **Use:** For analyzing videos by extracting both spatial and temporal features.\n",
        "- **Explanation:** By adding a time dimension to the convolutional layers, 3D CNNs can process sequences of frames, making them adept at recognizing actions and events in videos.\n",
        "- **Foundational Paper:** [3D Convolutional Neural Networks for Human Action Recognition by Ji et al.](https://ieeexplore.ieee.org/document/6165309)\n",
        "\n",
        "## Two-Stream Networks\n",
        "\n",
        "- **Use:** For a comprehensive analysis by considering both spatial and temporal information.\n",
        "- **Explanation:** This approach uses a dual-stream model, one for spatial features from single frames and another for temporal features from frame sequences, offering a balanced analysis.\n",
        "- **Foundational Paper:** [Two-stream convolutional networks for action recognition in videos by Simonyan & Zisserman.](https://arxiv.org/abs/1406.2199)\n",
        "\n",
        "\n",
        "## Transformers (e.g., Vision Transformers - ViT)\n",
        "\n",
        "- **Use:** For tasks where capturing long-range dependencies within videos is crucial.\n",
        "- **Explanation:** Adapting the attention mechanism from NLP, Vision Transformers process videos in a manner that emphasizes the interrelation of different parts of the video, both spatially and temporally.\n",
        "- **Foundational Papers:**\n",
        "  - General Transformers: [Attention is All You Need by Vaswani et al.](https://arxiv.org/abs/1706.03762)\n",
        "  - Vision Transformers: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "aCcq0xF_YBZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing Your Neural Architecture Template\n",
        "\n",
        "When selecting an architecture, consider the following:\n",
        "\n",
        "- **Nature of the Task:** Is your focus on understanding the content of individual frames (CNN), the movement between frames (RNN, 3D CNN), or a combination of both (Two-Stream, Transformers)?\n",
        "- **Complexity of the Video Data:** More complex data might benefit from architectures that can capture a wide range of dependencies, like Transformers.\n",
        "- **Computational Resources:** Some models, especially 3D CNNs and Transformers, are more computationally intensive than others.\n",
        "\n",
        "### Tips for Customization:\n",
        "\n",
        "- **Adjusting Parameters:** Tailor parameters like `frame_height`, `frame_width`, and `num_classes` according to your dataset.\n",
        "- **Preprocessing Needs:** Different architectures may require specific forms of input preprocessing. Ensure your data pipeline is compatible with your chosen model.\n",
        "\n",
        "After considering these aspects, select the architecture template that aligns with your project's goals from the list below. Each choice entails specific considerations for dataset dimensions and task specifications (e.g., classification, detection).\n",
        "\n",
        "1. **CNN for Spatial Features:** Ideal for projects focusing on frame-level analysis.\n",
        "2. **RNN/LSTM for Temporal Features:** Suited for understanding sequences and temporal patterns.\n",
        "3. **3D CNN for Spatio-Temporal Features:** Best for capturing actions and events over time.\n",
        "4. **Two-Stream Network:** Offers a comprehensive analysis by leveraging both spatial and temporal data.\n",
        "5. **Transformers:** For advanced projects requiring attention to complex patterns in large datasets.\n",
        "\n",
        "Remember to customize your model based on the specific needs of your dataset and the computational resources available to you."
      ],
      "metadata": {
        "id": "1snlAzZ-cxwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic CNN Model for Spatial Feature Extraction\n",
        "\n",
        "This CNN model is structured to extract spatial features from individual video frames, making it suitable for image-based analysis tasks within videos. Below is the template for creating a basic CNN:"
      ],
      "metadata": {
        "id": "KkaJRq9B8EMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic CNN Model for Spatial Feature Extraction\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Adjust these parameters to fit your dataset\n",
        "frame_height = 112  # Height of the video frame\n",
        "frame_width = 112   # Width of the video frame\n",
        "num_classes = 7    # Number of output classes\n",
        "\n",
        "# Basic CNN Model\n",
        "model_cnn = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(frame_height, frame_width, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "\n",
        "model_cnn.summary()\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-3SSfTJwYPlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN/LSTM for Temporal Feature Extraction\n",
        "\n",
        "This cell demonstrates how to set up a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) layers. It's designed for extracting temporal features from sequences of video frames, which is crucial for understanding activities, actions, or any phenomena that evolve over time."
      ],
      "metadata": {
        "id": "rsu1iTZT7G0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN/LSTM for Temporal Feature Extraction\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Adjust these parameters to fit your dataset\n",
        "timesteps = 100  # Length of your sequences\n",
        "features = 128   # Features extracted from each frame or timestep\n",
        "num_classes = 10 # Number of output classes\n",
        "\n",
        "# RNN Model with LSTM\n",
        "model_rnn = Sequential([\n",
        "    LSTM(64, input_shape=(timesteps, features), return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "\n",
        "model_rnn.summary()\n",
        "\n",
        "# Compile the model\n",
        "model_rnn.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Uc_PvCXraxti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3D CNN for Spatio-Temporal Feature Extraction\n",
        "\n",
        "This section introduces the setup for a 3D Convolutional Neural Network (3D CNN) designed to capture both spatial and temporal features from video clips. This model is capable of understanding motion and changes across consecutive frames, which is essential for tasks like action recognition.\n",
        "\n",
        "# 3D CNN for Spatio-Temporal Feature Extraction\n",
        "\n",
        "A 3D CNN extends the capabilities of traditional CNNs by analyzing sequences of frames to capture temporal dynamics alongside spatial features. Here's how to implement a 3D CNN model:"
      ],
      "metadata": {
        "id": "yKFCU0Os8M7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3D CNN for Spatio-Temporal Feature Extraction\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten\n",
        "\n",
        "# Adjust these parameters to fit your dataset\n",
        "frames_per_clip = 16    # Number of frames per video clip\n",
        "frame_height = 112      # Height of the video frame\n",
        "frame_width = 112       # Width of the video frame\n",
        "num_channels = 3        # Number of color channels (RGB)\n",
        "num_classes = 10        # Number of output classes\n",
        "\n",
        "# 3D CNN Model\n",
        "model_3dcnn = models.Sequential([\n",
        "    Conv3D(64, (3, 3, 3), activation='relu',\n",
        "           input_shape=(frames_per_clip, frame_height, frame_width, num_channels)),\n",
        "    MaxPooling3D((2, 2, 2)),\n",
        "    Conv3D(128, (3, 3, 3), activation='relu'),\n",
        "    MaxPooling3D((2, 2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "\n",
        "model_3dcnn.summary()\n",
        "\n",
        "# Compile the model\n",
        "model_3dcnn.compile(optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "WUSdQxatbGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two-Stream Networks for Video Analysis\n",
        "\n",
        "Implementing a two-stream network involves creating and training two separate models: one focused on spatial features (using a CNN) and another on temporal features (using either a 3D CNN or an RNN). The predictions from these models are then combined to produce a final output. This combination can be achieved through simple averaging or a more complex learned fusion layer.\n",
        "\n",
        "**Note:** This approach is conceptual, and specific implementation details will vary based on your project's needs.\n",
        "\n",
        "### Implementation Overview:\n",
        "\n",
        "- **For Spatial Features:** Utilize the CNN model outlined earlier.\n",
        "- **For Temporal Features:** Employ either the 3D CNN model or an RNN model, depending on the nature of your data and the specific temporal dynamics you wish to capture.\n",
        "- **Combining Models:** A straightforward method to combine these models is to average their predictions:"
      ],
      "metadata": {
        "id": "UCFBG3Xu4GwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of combining predictions\n",
        "predictions = 0.5 * cnn_model.predict(spatial_data) + 0.5 * temporal_model.predict(temporal_data)"
      ],
      "metadata": {
        "id": "0AZVrSh4bOdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vision Transformers for Video Processing\n",
        "\n",
        "Transformers have revolutionized natural language processing and are now making significant inroads into computer vision, including video processing tasks. Vision Transformers (ViT) apply the transformer architecture to image patches, treating each patch as a token similar to how words are treated in NLP. This method allows for capturing complex spatial hierarchies and has been extended to video processing to handle temporal dynamics as well.\n",
        "\n",
        "# Transformers for Video Processing (Vision Transformers - ViT)\n",
        "\n",
        "Vision Transformers (ViT) represent a novel approach in leveraging transformer architectures for video processing tasks. By decomposing video frames into a sequence of patches and processing these patches as tokens, ViTs can capture intricate spatial-temporal relationships within the video content.\n",
        "\n",
        "### Preliminary Steps:\n",
        "\n",
        "- **Base Model for Feature Extraction:** Leveraging pre-trained models like EfficientNet as a starting point for extracting features from video frames can be beneficial. These features then serve as inputs to the transformer model."
      ],
      "metadata": {
        "id": "3xxpEyRR6AMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers for Video Processing (Vision Transformers - ViT)\n",
        "\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from vit_keras import vit\n",
        "\n",
        "# Example for loading a pre-trained model\n",
        "base_model = EfficientNetB0(include_top=False, weights='imagenet')"
      ],
      "metadata": {
        "id": "4ToSvDVLbRUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Training and Monitoring the Model\n",
        "\n",
        "Training your machine learning model is a crucial step where the model learns to recognize patterns from the data. To track the model's progress and ensure that we save the best version, we'll employ callbacks like `ModelCheckpoint` and `TensorBoard`.\n"
      ],
      "metadata": {
        "id": "wTVzLBdDlt_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "import numpy as np\n",
        "\n",
        "# Loading the dataset splits\n",
        "data = np.load('/content/dataset_splits.npz') # Adjust according to your dataset path\n",
        "X_train = data['X_train']\n",
        "X_val = data['X_val']\n",
        "y_train = data['y_train']\n",
        "y_val = data['y_val']\n",
        "\n",
        "# Setup callbacks\n",
        "checkpoint_cb = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "tensorboard_cb = TensorBoard(log_dir='./logs')\n",
        "\n",
        "# Train the model\n",
        "history = model_cnn.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=12,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[checkpoint_cb, tensorboard_cb]\n",
        ")\n",
        "\n",
        "# Delete the dataset from Google Colab's disk to save RAM\n",
        "del data  # Delete the variable holding the dataset from memory\n",
        "\n",
        "# Optionally, delete the actual file from disk\n",
        "os.remove('/content/dataset_splits.npz')  # Adjust the path if necessary"
      ],
      "metadata": {
        "id": "dbPToovalwOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enabling TensorBoard within the notebook environment\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs"
      ],
      "metadata": {
        "id": "FFUXSfwo7My5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Hyperparameter Tuning\n",
        "\n",
        "Hyperparameter tuning is a pivotal phase in the machine learning pipeline. This process involves optimizing the model's architecture and the training procedure to maximize its performance. Hyperparameters, unlike the model's internal parameters learned during training, need to be set beforehand and have a substantial impact on the model's learning efficiency and output quality.\n",
        "\n",
        "Commonly tuned hyperparameters include:\n",
        "- **Learning Rate**: Controls how much to adjust the model in response to the estimated error each time the model weights are updated.\n",
        "- **Batch Size**: Number of training examples utilized in one iteration.\n",
        "- **Number of Epochs**: Total number of times the training dataset is passed forward and backward through the neural network.\n",
        "- **Architecture-Specific Parameters**: Such as the number of layers or units in a layer, which can vary significantly across different models.\n",
        "\n",
        "### Approaches to Hyperparameter Tuning:\n",
        "\n",
        "1. **Manual Tuning**: Relying on experience and intuition to adjust hyperparameters.\n",
        "2. **Grid Search**: Exhaustively searching through a predefined list of hyperparameter values.\n",
        "3. **Random Search**: Randomly selecting hyperparameter values from a defined range and evaluating their performance.\n",
        "4. **Bayesian Optimization**: Using probabilistic models to guide the search for the optimum hyperparameters.\n",
        "5. **Automated Tools**: Leveraging tools like Keras Tuner or Hyperopt to automate the tuning process.\n",
        "\n",
        "### Implementing Hyperparameter Tuning:\n",
        "\n",
        "For practical hyperparameter tuning, consider starting with either Grid Search or Random Search as they are straightforward to implement and can yield significant improvements:\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "mfOP6GAlS7aW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual Tuning Approach\n",
        "\n",
        "1. **Start with a baseline model**: Set up your model with a default set of hyperparameters.\n",
        "2. **Identify key hyperparameters**: Focus on those most likely to impact performance, such as learning rate or the number of layers.\n",
        "3. **Iteratively adjust values**: Manually change one hyperparameter at a time and monitor the effect on model performance.\n",
        "4. **Use a systematic approach**: Keep a log of changes and results to guide future adjustments.\n",
        "5. **Refinement**: Once the model shows improvement, refine your search around the best-performing values."
      ],
      "metadata": {
        "id": "wlCsjVKiAQah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search Template\n",
        "\n",
        "Grid Search exhaustively tests a predefined range of hyperparameter values, ensuring that you explore all possible combinations within your specified grid. This method is particularly useful when the number of hyperparameters and their potential values are relatively low."
      ],
      "metadata": {
        "id": "hd3YJqPaFre4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Assume 'build_model' is a function that constructs a Keras model\n",
        "def build_model(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_shape=(input_dim,), activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=build_model)\n",
        "\n",
        "param_grid = {\n",
        "    'epochs': [10, 20],\n",
        "    'batch_size': [16, 32],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Access the best set of parameters\n",
        "best_params = grid_result.best_params_"
      ],
      "metadata": {
        "id": "NUnBYKDEAjea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Search Template\n",
        "\n",
        "Random Search optimizes hyperparameters by sampling values from a defined distribution. This method can be more efficient than Grid Search, particularly when the hyperparameter space is large."
      ],
      "metadata": {
        "id": "nLjTu-32F_Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "def build_model(optimizer='adam'):\n",
        "    # Model construction (omitted for brevity)\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=build_model, epochs=20, batch_size=32)\n",
        "\n",
        "param_dist = {\n",
        "    'batch_size': [16, 32, 64],\n",
        "    'epochs': [10, 20, 30],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, n_jobs=-1, cv=3)\n",
        "random_search_result = random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_params = random_search_result.best_params_"
      ],
      "metadata": {
        "id": "35b3pOd3FMtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bayesian Optimization Template\n",
        "\n",
        "Bayesian Optimization leverages a probabilistic model to select hyperparameters that are likely to yield better results. This method is efficient for finding optimal hyperparameters with fewer trials, making it ideal for complex models."
      ],
      "metadata": {
        "id": "BqrmAtNxGMPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def objective(space):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=int(space['units']), input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(learning_rate=space['learning_rate']),\n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=int(space['batch_size']), verbose=0)\n",
        "\n",
        "    val_loss = np.min(history.history['val_loss'])\n",
        "    return {'loss': val_loss, 'status': STATUS_OK}\n",
        "\n",
        "space = {\n",
        "    'units': hp.quniform('units', 50, 150, 1),\n",
        "    'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
        "}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(fn=objective,\n",
        "            space=space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=100,\n",
        "            trials=trials)\n",
        "\n",
        "best_params = space_eval(space, best)"
      ],
      "metadata": {
        "id": "MaBKp3nZFSUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning with Keras Tuner\n",
        "\n",
        "Keras Tuner simplifies the task of finding the best hyperparameters for your model. The example below outlines how to use Keras Tuner to optimize a Convolutional Neural Network (CNN) architecture."
      ],
      "metadata": {
        "id": "9L7zyrb--run"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_tuner import RandomSearch\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "\n",
        "input_shape = (112, 112, 3)  # Example input shape; adjust as necessary\n",
        "num_classes = 10  # Adjust based on your dataset\n",
        "\n",
        "# Function to build the model (required for Keras Tuner)\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters=hp.Int('conv_filters', min_value=32, max_value=128, step=32),\n",
        "                     kernel_size=hp.Choice('conv_kernel_size', values=[3, 5]),\n",
        "                     activation='relu',\n",
        "                     input_shape=input_shape))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=128, step=32),\n",
        "                    activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='hparam_tuning'\n",
        ")\n",
        "\n",
        "# Perform the search\n",
        "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Retrieve the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "print(f\"\"\"\n",
        "The optimal number of units in the first dense layer is {best_hps.get('dense_units')} and the\n",
        "optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n",
        "\n",
        "# Rebuild the model with the best hyperparameters and train it\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "IAj1IU7JYyga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Evaluating the Model\n",
        "\n",
        "Once your model has been trained and tuned, the next step is to evaluate its performance on the test set. This is crucial for understanding how well the model can generalize to new, unseen data. Here's how you can evaluate your model using TensorFlow/Keras:"
      ],
      "metadata": {
        "id": "uFSCmiR4mAoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming y_test is a list or a Pandas series, convert it to a numpy array for compatibility with Keras\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_cnn.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.2f}\")"
      ],
      "metadata": {
        "id": "jq40-ytbmBSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Implementing Regularization Techniques\n",
        "\n",
        "Regularization techniques are critical in preventing overfitting, especially when you have a high-capacity model or limited data. Overfitting occurs when a model learns the noise in the training data to the extent that it negatively impacts the performance of the model on new data. Regularization methods provide ways to penalize model complexity or introduce noise to the training process to promote the generalizability of the model.\n",
        "\n",
        "There are several regularization techniques:\n",
        "- **L1 and L2 Regularization**: Penalizes the weights of the model during training, which can help to prevent overfitting by encouraging simpler models that may generalize better.\n",
        "- **Dropout**: Randomly sets a fraction of input units to 0 at each update during training time, which helps to prevent overfitting by making the neural network less sensitive to the specific weights of neurons.\n",
        "- **Batch Normalization**: Although primarily used to normalize the input layer by adjusting and scaling the activations, it can also have a regularizing effect.\n",
        "- **Early Stopping**: Monitors the model's performance on a validation set and stops training when performance begins to degrade.\n",
        "\n",
        "In the following code cells, we will add L2 regularization and dropout to our neural network to help with overfitting. We'll also implement early stopping to halt the training process at the optimal moment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QpMbGniVTMOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Template for Building a Regularized Model\n",
        "\n",
        "Below is a template function `build_regularized_model()` that defines a neural network model incorporating L2 regularization and dropout for improved generalization.\n"
      ],
      "metadata": {
        "id": "oVt_itLv9ATK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers, regularizers\n",
        "\n",
        "frame_height = 112  # Adjust based on your data\n",
        "frame_width = 112   # Adjust based on your data\n",
        "num_classes = 10    # Adjust based on your data\n",
        "\n",
        "def build_regularized_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(frame_height, frame_width, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "7ISEsY5o9Vdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Employ Early Stopping During Training\n",
        "\n",
        "Use the EarlyStopping callback to halt training when the validation loss stops improving, helping to prevent overfitting on the training data.\n",
        "\n",
        "\n",
        "## Training with Early Stopping\n",
        "\n",
        "To utilize Early Stopping, include it in the callbacks when training your model. This approach stops the training process early if the model's performance on the validation set does not improve, helping to save resources and prevent overfitting."
      ],
      "metadata": {
        "id": "500jKaIg9fnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Setting up EarlyStopping\n",
        "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Training the model with the regularization techniques and early stopping\n",
        "history_regularized = model_regularized.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,  # High epoch limit, but training may stop early due to early stopping\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping_cb]  # Make sure to include other callbacks if needed\n",
        ")"
      ],
      "metadata": {
        "id": "cLRRa00d9y9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Evaluate the Model\n",
        "\n",
        "After training, evaluate the regularized model on the test set to assess its performance on unseen data.\n",
        "\n",
        "## Model Evaluation\n",
        "\n",
        "Finally, assess how well your regularized model generalizes by evaluating it on the test set."
      ],
      "metadata": {
        "id": "ohEWBf1y92tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_reg, test_acc_reg = model_regularized.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy with regularization: {test_acc_reg:.2f}\")"
      ],
      "metadata": {
        "id": "Oh1N55_R-ATH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Preparing for Fine-Tuning\n",
        "\n",
        "Fine-tuning enhances a pre-trained model's performance on a new dataset by carefully adjusting its parameters. The key steps involve freezing certain layers of the model while allowing others to update, and setting a lower learning rate for subtle adjustments."
      ],
      "metadata": {
        "id": "n9YozC24nyUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Assuming 'model' is a pre-trained model and 'layer_to_freeze' is the layer up to which you want to freeze\n",
        "\n",
        "# Freeze layers not intended for fine-tuning\n",
        "for layer in model.layers[:layer_to_freeze]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Unfreeze layers for fine-tuning\n",
        "for layer in model.layers[layer_to_freeze:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Adjust the learning rate for fine-tuning\n",
        "optimizer = Adam(learning_rate=1e-5)  # Set a lower learning rate for fine-tuning\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Review the model structure to confirm layer adjustments\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "lGkv5IT1ny87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning the Model\n",
        "\n",
        "After preparing your model for fine-tuning, proceed with training, focusing on refining the model's understanding of your specific dataset. Monitoring the model's performance during this phase is crucial to ensure that it improves."
      ],
      "metadata": {
        "id": "ZggZqLKp_Vdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue the training process with fine-tuning adjustments\n",
        "history_fine = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,  # Number of epochs may be adjusted based on observed performance improvements\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[checkpoint_cb, tensorboard_cb]  # Utilize callbacks from initial training for consistency\n",
        ")"
      ],
      "metadata": {
        "id": "pnhidTZwoB4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Evaluating the Model Post Fine-Tuning\n",
        "\n",
        "Once fine-tuning is complete, it's crucial to evaluate the model's performance again. This allows you to measure the effectiveness of the fine-tuning process and understand how it has impacted the model's ability to generalize to unseen data.\n"
      ],
      "metadata": {
        "id": "MreWcG6_ol7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'model_cnn' is your model after fine-tuning\n",
        "test_loss_ft, test_acc_ft = model_cnn.evaluate(X_test, y_test)\n",
        "print(f\"Post Fine-Tuning Test Accuracy: {test_acc_ft:.2f}\")"
      ],
      "metadata": {
        "id": "DfzE2yYZonpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Saving the Trained Model\n",
        "\n",
        "After your model has been trained and fine-tuned to your satisfaction, saving it allows for easy reuse and deployment. The following snippet demonstrates how to save your TensorFlow/Keras model as an H5 file, which is a portable format for storing models."
      ],
      "metadata": {
        "id": "5CfDUZjtq3fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Assuming 'model_cnn' is your final model after training and fine-tuning\n",
        "model_cnn.save('my_model.h5')\n",
        "\n",
        "# Verification step to check if the model has been saved correctly\n",
        "if os.path.exists('my_model.h5'):\n",
        "    print('Model saved successfully as my_model.h5')\n",
        "else:\n",
        "    print('Model saving failed.')"
      ],
      "metadata": {
        "id": "519WwJCvq4Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading and Inference"
      ],
      "metadata": {
        "id": "cYRh1nzRTlgW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CF5DeERgT3bO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}