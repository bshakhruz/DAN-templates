{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK9MNhgKHd+7C0gzCYb1aK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bshakhruz/DAN-templates/blob/main/DNN_video_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Deep Neural Networks on Videos Using Google Colab\n"
      ],
      "metadata": {
        "id": "vw7kfwB_Ond4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction to Google Colab\n",
        "\n",
        "Google Colab is a free cloud service based on Jupyter Notebooks that supports free GPU and TPU usage. It's ideal for machine learning, data analysis, and education. The platform eliminates the need for expensive hardware, making deep learning more accessible.\n",
        "\n"
      ],
      "metadata": {
        "id": "W1X96z7mRv8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setting up the Colab Environment\n",
        "\n",
        "- **Create a New Notebook**: Open Google Colab, click on 'New Notebook' to start.\n",
        "- **Enable GPU/TPU**: Go to `Edit` > `Notebook Settings` or `Runtime` > `Change runtime type` and select GPU or TPU from the dropdown to accelerate your computations.\n"
      ],
      "metadata": {
        "id": "kuUXcEX2Qp67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Installing Dependencies\n",
        "!pip install tensorflow opencv-python-headless pytorch torchvision torchaudio"
      ],
      "metadata": {
        "id": "DIztlvGSSF0y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Accessing and Preparing Video Data\n",
        "\n",
        "To work with video data in Google Colab, you can upload video files directly to Colab or access them via Google Drive. Once the videos are accessible, you'll use OpenCV to preprocess them, such as extracting frames and normalizing pixel values. This step is crucial for converting raw video files into a structured format that can be used for training deep learning models. The goal is to organize your preprocessed video data into training, validation, and test sets.\n"
      ],
      "metadata": {
        "id": "1wenL1i1UIEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Additional Dependencies for Preprocessing Phase\n",
        "!apt update\n",
        "!apt install ffmpeg\n",
        "!pip install moviepy"
      ],
      "metadata": {
        "id": "mnxbVB1-SOnF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive (Optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i8jHmSoVUREH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This snippet loads a video, extract frames, and preprocess them,\n",
        "# NOTE: place your actual path in the 'video_path' variable\n",
        "\n",
        "# Necessary library imports\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to extract and preprocess frames\n",
        "def extract_and_preprocess_frames(video_path):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # Preprocess steps (e.g., resizing, normalization)\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        resized_frame = cv2.resize(frame_rgb, (224, 224))  # Resize frame to model input size\n",
        "        frames.append(resized_frame / 255.0)  # Normalize pixel values\n",
        "    cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "# Example usage for a single video\n",
        "video_path = '/content/drive/My Drive/path_to_your_video.mp4' # Adjust this path\n",
        "processed_frames = extract_and_preprocess_frames(video_path)\n",
        "\n",
        "# Optionally, display a frame to verify preprocessing\n",
        "plt.imshow(processed_frames[0])\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FnsfWYTtVGON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Organizing the dataset\n",
        "# Assuming 'processed_frames' contains all your preprocessed video frames\n",
        "# and 'labels' is an array of corresponding labels for each video.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the dataset into training, validation, and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_frames, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the training set to create a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # Adjust test_size as per your requirement\n",
        "\n",
        "# Note: Adjust the test_size parameter based on how much data you want to allocate for testing and validation.\n",
        "# Now X_train, X_val, and X_test along with y_train, y_val, and y_test are ready to be used in the model training process."
      ],
      "metadata": {
        "id": "U3QeNaW8h5_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Overview of Different Architectures for Video Analysis\n",
        "\n",
        "Selecting the appropriate architecture for video analysis is crucial for achieving good performance. Here's an overview of the popular architectures and their primary applications:\n",
        "\n",
        "### CNNs (Convolutional Neural Networks)\n",
        "\n",
        "- **Use**: Best for extracting spatial features from individual video frames.\n",
        "- **Explanation**: CNNs are adept at recognizing patterns, shapes, and objects within images, making them perfect for analyzing static frames.\n",
        "\n",
        "### RNN/LSTM/GRU\n",
        "\n",
        "- **Use**: Ideal for capturing temporal dependencies in sequential data, like videos.\n",
        "- **Explanation**: RNNs (Recurrent Neural Networks) and their advanced variants, LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Units), can model time-based dynamics, crucial for understanding activities and events in videos.\n",
        "\n",
        "### 3D CNNs\n",
        "\n",
        "- **Use**: For simultaneous spatial and temporal feature extraction.\n",
        "- **Explanation**: 3D CNNs extend conventional CNNs by adding a time dimension, allowing them to process video clips as volumetric data, capturing motion information directly.\n",
        "\n",
        "### Two-Stream Networks\n",
        "\n",
        "- **Use**: Combines spatial and temporal streams for comprehensive video analysis.\n",
        "- **Explanation**: This architecture uses one CNN stream to process single frames for spatial features, and another stream, often an RNN or 3D CNN, to process the motion between frames, offering a balance between the two types of information.\n",
        "\n",
        "### Transformers (e.g., Vision Transformers - ViT)\n",
        "\n",
        "- **Use**: Recently, transformers have been adapted for video processing tasks.\n",
        "- **Explanation**: Transformers, known for their effectiveness in NLP, have been adapted for video through architectures like ViT, allowing for attention-based mechanisms to capture long-range dependencies in both space and time within videos."
      ],
      "metadata": {
        "id": "aCcq0xF_YBZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose Your Neural Architecture Template\n",
        "\n",
        "After familiarizing yourself with the different architectures available for video analysis, select the one that best fits your project's needs from the options provided below. Ensure to adjust parameters such as `frame_height`, `frame_width`, and `num_classes` to match your dataset specifics. Once you've made your selection, run the corresponding cell to define and compile your model.\n",
        "\n",
        "- **CNN for Spatial Features**: Best for analyzing frame-level features.\n",
        "- **RNN/LSTM for Temporal Features**: Ideal for understanding temporal dynamics.\n",
        "- **3D CNN for Spatio-Temporal Features**: Captures both spatial and temporal information.\n",
        "- **Two-Stream Network**: Combines CNN and RNN strengths for comprehensive analysis.\n",
        "- **Transformers**: Utilizes advanced attention mechanisms for complex patterns.\n",
        "\n",
        "\n",
        "**Note**: Each architecture requires specific adjustments related to your dataset dimensions and the problem you are solving (e.g., classification, detection). Modify `frame_height`, `frame_width`, `num_classes`, and any other relevant parameters before running your chosen architecture cell."
      ],
      "metadata": {
        "id": "1snlAzZ-cxwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic CNN Model for Spatial Feature Extraction\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Adjust these parameters to fit your dataset\n",
        "frame_height = 224  # Height of the video frame\n",
        "frame_width = 224   # Width of the video frame\n",
        "num_classes = 10    # Number of output classes\n",
        "\n",
        "# Basic CNN Model\n",
        "model_cnn = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(frame_height, frame_width, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "\n",
        "model_cnn.summary()\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-3SSfTJwYPlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN/LSTM for Temporal Feature Extraction\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Adjust these parameters to fit your dataset\n",
        "timesteps = 100  # Length of your sequences\n",
        "features = 128   # Features extracted from each frame or timestep\n",
        "num_classes = 10 # Number of output classes\n",
        "\n",
        "# RNN Model with LSTM\n",
        "model_rnn = Sequential([\n",
        "    LSTM(64, input_shape=(timesteps, features), return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "\n",
        "model_rnn.summary()\n",
        "\n",
        "# Compile the model\n",
        "model_rnn.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Uc_PvCXraxti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3D CNN for Spatio-Temporal Feature Extraction\n",
        "\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten\n",
        "\n",
        "# Adjust these parameters to fit your dataset\n",
        "frames_per_clip = 16    # Number of frames per video clip\n",
        "frame_height = 112     # Height of the video frame\n",
        "frame_width = 112      # Width of the video frame\n",
        "num_channels = 3       # Number of color channels (RGB)\n",
        "num_classes = 10       # Number of output classes\n",
        "\n",
        "# 3D CNN Model\n",
        "model_3dcnn = models.Sequential([\n",
        "    Conv3D(64, (3, 3, 3), activation='relu',\n",
        "           input_shape=(frames_per_clip, frame_height, frame_width, num_channels)),\n",
        "    MaxPooling3D((2, 2, 2)),\n",
        "    Conv3D(128, (3, 3, 3), activation='relu'),\n",
        "    MaxPooling3D((2, 2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "\n",
        "model_3dcnn.summary()\n",
        "\n",
        "# Compile the model\n",
        "model_3dcnn.compile(optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "WUSdQxatbGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: Implementing a two-stream network involves creating and training two separate models:\n",
        "# one for spatial features (e.g., a CNN) and one for temporal features (e.g., a 3D CNN or an RNN).\n",
        "# After training, their predictions are typically combined through averaging or a learned fusion layer.\n",
        "\n",
        "# This cell is meant to guide you conceptually; implementation details will vary based on your specific needs.\n",
        "\n",
        "# For Spatial Features: Use the CNN model from Cell 1.\n",
        "# For Temporal Features: Use the 3D CNN model from Cell 3 or an RNN model depending on your preference.\n",
        "\n",
        "# The final step involves combining these models. One simple approach is to average their predictions:\n",
        "# predictions = 0.5 * cnn_model.predict(spatial_data) + 0.5 * temporal_model.predict(temporal_data)"
      ],
      "metadata": {
        "id": "0AZVrSh4bOdg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers for Video Processing (Vision Transformers - ViT)\n",
        "\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from vit_keras import vit\n",
        "\n",
        "# Adjust these parameters"
      ],
      "metadata": {
        "id": "4ToSvDVLbRUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Monitoring the Model\n",
        "\n",
        "Now that your model is ready, it's time to train it with your dataset. Use the `.fit()` method for training. To monitor the training progress and ensure the best model is saved, we'll use callbacks such as `ModelCheckpoint` and `TensorBoard`."
      ],
      "metadata": {
        "id": "wTVzLBdDlt_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "\n",
        "# Setup callbacks\n",
        "checkpoint_cb = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "tensorboard_cb = TensorBoard(log_dir='./logs')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[checkpoint_cb, tensorboard_cb]\n",
        ")"
      ],
      "metadata": {
        "id": "dbPToovalwOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model\n",
        "\n",
        "After training, it's crucial to evaluate your model on the test set to understand its performance on unseen data. This step gives you insights into how well your model has learned and generalized from the training data.\n"
      ],
      "metadata": {
        "id": "uFSCmiR4mAoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.2f}\")"
      ],
      "metadata": {
        "id": "jq40-ytbmBSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing for Fine-Tuning (if needed)\n",
        "\n",
        "To fine-tune your model, identify which layers need adjustment and set a lower learning rate for fine-tuning. This approach delicately refines the model's ability to adapt to your specific dataset."
      ],
      "metadata": {
        "id": "n9YozC24nyUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for a model with a pre-trained base\n",
        "for layer in model.layers[:layer_to_freeze]:\n",
        "    layer.trainable = False  # Freeze layers not intended for fine-tuning\n",
        "for layer in model.layers[layer_to_freeze:]:\n",
        "    layer.trainable = True  # Unfreeze layers for fine-tuning\n",
        "\n",
        "# Adjust the learning rate for fine-tuning\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)  # Lower learning rate\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summarize the model post adjustments to verify changes\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "lGkv5IT1ny87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-Tuning the Model With the model adjusted for fine-tuning,\n",
        "# continue training to refine its performance on the dataset. Monitor the training process closely to ensure improvements.\n",
        "\n",
        "# Continue training the model\n",
        "history_fine = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,  # Adjust epochs based on when performance plateaus\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[checkpoint_cb, tensorboard_cb]  # Reuse callbacks from initial training\n",
        ")"
      ],
      "metadata": {
        "id": "pnhidTZwoB4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model Post Fine-Tuning\n",
        "\n",
        "After fine-tuning, evaluate the model again on the test set to assess any improvements. This step helps understand the effectiveness of your fine-tuning efforts.\n"
      ],
      "metadata": {
        "id": "MreWcG6_ol7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model again\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Post Fine-Tuning Test Accuracy: {test_acc:.2f}\")"
      ],
      "metadata": {
        "id": "DfzE2yYZonpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Trained Model\n",
        "\n",
        "After training and fine-tuning, save your model to reuse it later without needing to retrain. This step is crucial for deployment."
      ],
      "metadata": {
        "id": "5CfDUZjtq3fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model to a file\n",
        "model.save('my_model.h5')\n",
        "\n",
        "# Note: TensorFlow also supports saving in the SavedModel format using 'save('my_model')'"
      ],
      "metadata": {
        "id": "519WwJCvq4Cs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}