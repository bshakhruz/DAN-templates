{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "uFSCmiR4mAoN",
        "MreWcG6_ol7x"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNiJjtpgemfhWlWkvSNW2Zg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bshakhruz/DAN-templates/blob/main/DNN_video_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Deep Neural Networks on Videos Using Google Colab\n",
        "---\n"
      ],
      "metadata": {
        "id": "vw7kfwB_Ond4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction to Google Colab\n",
        "\n",
        "Google Colab is a free cloud service based on Jupyter Notebooks that supports free GPU and TPU usage. It's ideal for machine learning, data analysis, and education. The platform eliminates the need for expensive hardware, making deep learning more accessible.\n",
        "\n"
      ],
      "metadata": {
        "id": "W1X96z7mRv8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Setting up the Colab Environment\n",
        "\n",
        "- **Enable GPU/TPU**: Go to `Edit` > `Notebook Settings` or `Runtime` > `Change runtime type` and select GPU or TPU from the dropdown to accelerate your computations.\n",
        "\n",
        "**Checkpoint:** Remember to save your notebook now to preserve this setting.\n",
        "\n",
        "## Verifying the Accelerator Status\n",
        "\n",
        "After enabling the hardware accelerator, run the following cell to confirm that it is active:\n"
      ],
      "metadata": {
        "id": "kuUXcEX2Qp67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the hardware accelerator (GPU/TPU)\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    print('No GPU found')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "8hq3eWVtq5Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Installing Dependencies\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install tensorflow opencv-python-headless\n",
        "!pip3 install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "DIztlvGSSF0y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fa4c132-5ac9-4e0a-a6b5-e8c53c93b5c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-69.2.0-py3-none-any.whl (821 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.5/821.5 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.43.0)\n",
            "Installing collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-24.0 setuptools-69.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              },
              "id": "aceb8ec604f74e1daa16b45cf1066c5b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (69.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.0/410.6 MB\u001b[0m \u001b[31m210.0 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[2KTraceback (most recent call last):\n",
            "\u001b[2K  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 180, in \n",
            "exc_logging_wrapper\n",
            "\u001b[2K    status = run_func(*args)\n",
            "\u001b[2K  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 245, in \n",
            "wrapper\n",
            "\u001b[2K    return func(self, options, args)\n",
            "\u001b[2K  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 377, in run\n",
            "\u001b[2K    requirement_set = resolver.resolve(\n",
            "\u001b[2K  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/resolution/resolvelib/resolver.py\", \n",
            "line 179, in resolve\n",
            "\u001b[2K    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
            "\u001b[2K  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 552, in \n",
            "prepare_linked_requirements_more\n",
            "\u001b[2K    self._complete_partial_requirements(\n",
            "\u001b[2K  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/prepare.py\", line 467, in \n",
            "_complete_partial_requirements\n",
            "\u001b[2K    for link, (filepath, _) in batch_download:\n",
            "\u001b[2K  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/network/download.py\", line 184, in \n",
            "__call__\n",
            "\u001b[2K    content_file.write(chunk)\n",
            "\u001b[2KKeyboardInterrupt\n",
            "\u001b[2K\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\u001b[2KTraceback (most recent call last):\n",
            "\u001b[2K  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "\u001b[2K    sys.exit(main())\n",
            "\u001b[2K  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "\u001b[2K    return command.main(cmd_args)\n",
            "\u001b[2K  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in \n",
            "main\n",
            "\u001b[2K    return self._main(args)\n",
            "\u001b[2K  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 234, in \n",
            "_main\n",
            "\u001b[2K    return run(options, args)\n",
            "\u001b[2K  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 218, in \n",
            "exc_logging_wrapper\n",
            "\u001b[2K    logger.debug(\"Exception information:\", exc_info=True)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/logging/__init__.py\", line 1465, in debug\n",
            "\u001b[2K    self._log(DEBUG, msg, args, **kwargs)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/logging/__init__.py\", line 1624, in _log\n",
            "\u001b[2K    self.handle(record)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/logging/__init__.py\", line 1634, in handle\n",
            "\u001b[2K    self.callHandlers(record)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/logging/__init__.py\", line 1696, in callHandlers\n",
            "\u001b[2K    hdlr.handle(record)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/logging/__init__.py\", line 968, in handle\n",
            "\u001b[2K    self.emit(record)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/logging/handlers.py\", line 75, in emit\n",
            "\u001b[2K    logging.FileHandler.emit(self, record)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/logging/__init__.py\", line 1218, in emit\n",
            "\u001b[2K    StreamHandler.emit(self, record)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
            "\u001b[2K    msg = self.format(record)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/logging/__init__.py\", line 943, in format\n",
            "\u001b[2K    return fmt.format(record)\n",
            "\u001b[2K  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "\u001b[2K    formatted = super().format(record)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/logging/__init__.py\", line 686, in format\n",
            "\u001b[2K    record.exc_text = self.formatException(record.exc_info)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/logging/__init__.py\", line 636, in formatException\n",
            "\u001b[2K    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/traceback.py\", line 119, in print_exception\n",
            "\u001b[2K    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/traceback.py\", line 502, in __init__\n",
            "\u001b[2K    self.stack = StackSummary.extract(\n",
            "\u001b[2K  File \"/usr/lib/python3.10/traceback.py\", line 370, in extract\n",
            "\u001b[2K    linecache.lazycache(filename, f.f_globals)\n",
            "\u001b[2K  File \"/usr/lib/python3.10/linecache.py\", line 176, in lazycache\n",
            "\u001b[2K    get_source = getattr(loader, 'get_source', None)\n",
            "\u001b[2KKeyboardInterrupt\n",
            "\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.5/410.6 MB\u001b[0m \u001b[31m187.7 MB/s\u001b[0m eta \u001b[36m0:00:03\u001b[0m\n",
            "\u001b[?25h^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify installation\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import torch\n",
        "print(f'TensorFlow version: {tf.__version__}')\n",
        "print(f'OpenCV version: {cv2.__version__}')\n",
        "print(f'PyTorch version: {torch.__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpmCICN0rJ2D",
        "outputId": "9f4a5bbf-a047-4eb4-b42f-03e7b0895f3d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n",
            "OpenCV version: 4.8.0\n",
            "PyTorch version: 2.2.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Accessing and Preparing Video Data\n",
        "\n",
        "Training deep learning models on video data requires organizing and preprocessing your video files into a format that your model can process. This section guides you through accessing your video data, whether stored locally or on Google Drive, and details the preprocessing steps necessary to prepare the data for model training.\n",
        "\n",
        "## Preparing the Environment\n",
        "\n",
        "Before you start preprocessing your video data, you'll need to install some additional dependencies.\n"
      ],
      "metadata": {
        "id": "1wenL1i1UIEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing Additional Dependencies for Preprocessing Phase\n",
        "!apt update && !apt install ffmpeg\n",
        "!pip install moviepy"
      ],
      "metadata": {
        "id": "mnxbVB1-SOnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9f9293-c0d8-470e-edbf-a582b97f2e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,920 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,357 kB]\n",
            "Fetched 3,510 kB in 4s (932 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive (Optional)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "i8jHmSoVUREH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This snippet loads a video, extract frames, and preprocess them,\n",
        "# NOTE: place your actual path in the 'video_path' variable\n",
        "\n",
        "# Necessary library imports\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Placeholder for directory where videos are located\n",
        "video_directory = '<your_video_directory>'\n",
        "\n",
        "# List all video files in the directory\n",
        "video_files = [file for file in os.listdir(video_directory) if file.endswith('.mp4')]\n",
        "\n",
        "# Placeholder for labeling videos (replace with your own labels)\n",
        "video_labels = {\n",
        "    'video1.mp4': 0,\n",
        "    'video2.mp4': 1,\n",
        "    # Add as many videos as you have...\n",
        "}\n",
        "# Function to extract and preprocess frames\n",
        "def extract_and_preprocess_frames(video_path):\n",
        "    skip_frames=5\n",
        "    count=0\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count % skip_frames == 0:\n",
        "            # Preprocess steps (e.g., resizing, normalization)\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            resized_frame = cv2.resize(frame_rgb, (112, 112))  # Resize frame to model input size\n",
        "            frames.append(resized_frame / 255.0)  # Normalize pixel values\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return np.array(frames)\n",
        "\n",
        "# Process each video file\n",
        "all_frames = []\n",
        "for video_file in video_files:\n",
        "    video_path = os.path.join(video_directory, video_file)\n",
        "    processed_frames = extract_and_preprocess_frames(video_path)\n",
        "    print(f\"Processed {len(processed_frames)} frames from {video_file}\")\n",
        "    all_frames.append(processed_frames)  # Assuming you want to keep frames of all videos in one list\n",
        "\n",
        "labels = []\n",
        "for video_file, processed_frames in zip(video_files, all_frames):\n",
        "    label = video_labels[video_file]\n",
        "    # Extend the labels list with repeated labels for the number of frames in each video\n",
        "    labels.extend([label] * len(processed_frames))\n",
        "\n",
        "# Ensure processed_frames is correctly flattened\n",
        "processed_frames = np.concatenate(all_frames, axis=0)  # This should give you a flat array of all frames\n",
        "\n",
        "\n",
        "# After attempting to add all frames\n",
        "print(f\"Total videos processed: {len(all_frames)}\")\n",
        "if all_frames:\n",
        "    print(f\"Frames in first video: {len(all_frames[0])}\")\n",
        "else:\n",
        "    print(\"No videos were processed.\")\n",
        "if all_frames and len(all_frames[0]) > 0:\n",
        "    # Optionally, display a frame to verify preprocessing\n",
        "    plt.imshow(all_frames[0][0])\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No frames available to display.\")"
      ],
      "metadata": {
        "id": "FnsfWYTtVGON",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "86f63da6-9627-4d6a-ddb8-0a0c9b01e004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 550 frames from Mr Bean (The Angriest Nanny Ever).mp4\n",
            "Processed 528 frames from Indoor Surfing _ Mr. Bean Cartoon.mp4\n",
            "Processed 234 frames from Peppa pig took over Mr Bean Intro Funny edited.mp4\n",
            "Processed 525 frames from Magpie Hospital _ Funny Clip _ Mr. Bean Cartoon.mp4\n",
            "Processed 448 frames from Mr Bean - Bad Customer Service.mp4\n",
            "Processed 231 frames from Mr. Bean Animated Series Theme Song [HD].mp4\n",
            "Processed 574 frames from Car chase for bottle _ Funny Clip _ Mr. Bean Official Cartoon.mp4\n",
            "Total videos processed: 7\n",
            "Frames in first video: 550\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Organizing the Dataset\n",
        "\n",
        "After preprocessing the video data, the next step is to organize the dataset into distinct sets for training, validation, and testing. This ensures that we can train our model, fine-tune hyperparameters, and evaluate performance effectively.\n",
        "\n",
        "## Splitting the Dataset\n",
        "\n",
        "We will use `train_test_split` from `sklearn` to partition the data. The `test_size` parameter determines the proportion of the dataset to include in the test split."
      ],
      "metadata": {
        "id": "-fLvTwYivbIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'processed_frames' contains all your preprocessed video frames\n",
        "# and 'labels' is an array of corresponding labels for each video.\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    processed_frames, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Further split the training set to create a validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.25, random_state=42\n",
        ")  # 0.25 x 0.8 = 0.2 of the original dataset # Adjust test_size as per your requirement\n",
        "\n",
        "\n",
        "# Note: Adjust the test_size parameter based on how much data you want to allocate for testing and validation.\n",
        "# Now X_train, X_val, and X_test along with y_train, y_val, and y_test are ready to be used in the model training process.\n",
        "\n",
        "# Save arrays to .npz file\n",
        "np.savez('/content/dataset_splits.npz', X_train=X_train, X_val=X_val, X_test=X_test, y_train=y_train, y_val=y_val, y_test=y_test)"
      ],
      "metadata": {
        "id": "U3QeNaW8h5_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Data Splits\n",
        "import numpy as np\n",
        "\n",
        "# Replace '/content/dataset_splits.npz' with your preferred save path\n",
        "np.savez(\n",
        "    '/content/dataset_splits.npz',\n",
        "    X_train=X_train, X_val=X_val, X_test=X_test,\n",
        "    y_train=y_train, y_val=y_val, y_test=y_test\n",
        ")"
      ],
      "metadata": {
        "id": "D-SNmEnCwej_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Data Augmentation (Optional)\n",
        "\n",
        "Data augmentation is an essential technique in the machine learning workflow, particularly when dealing with image and video data. By applying random transformations to your training data, you can artificially expand the size and variance of your dataset. This process is key to preventing overfitting and helps the model generalize better to new, unseen data.\n",
        "\n",
        "In this section, we'll use Keras's preprocessing layers to implement on-the-fly data augmentation.\n",
        "\n",
        "## Why Data Augmentation?\n",
        "\n",
        "- **Improves Generalization:** By simulating a broader set of variations, the model is less likely to memorize specific data points.\n",
        "- **Addresses Overfitting:** Especially in scenarios with limited data, augmentation can effectively increase the dataset size.\n",
        "- **Enhances Robustness:** Models trained with augmented data often perform better in real-world scenarios where data imperfections are common.\n",
        "\n",
        "## Implementing Data Augmentation\n",
        "\n",
        "The `ImageDataGenerator` class in Keras provides a suite of tools for on-the-fly image augmentation."
      ],
      "metadata": {
        "id": "6gd6OmF4L2RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define your data augmentation pipeline\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,        # Random rotations from 0 to 40 degrees\n",
        "    width_shift_range=0.2,    # Random horizontal shifts\n",
        "    height_shift_range=0.2,   # Random vertical shifts\n",
        "    shear_range=0.2,          # Shear transformations\n",
        "    zoom_range=0.2,           # Random zoom\n",
        "    horizontal_flip=True,     # Random horizontal flips\n",
        "    fill_mode='nearest'       # Strategy for filling in new pixels\n",
        ")\n",
        "\n",
        "# Visualization of Data Augmentation\n",
        "# Let's visualize some augmented examples to ensure our transformations are correct.\n",
        "x_sample = X_train[0]\n",
        "y_sample = y_train[0]\n",
        "\n",
        "# Generate and plot a batch of augmented images\n",
        "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "axes = axes.flatten()\n",
        "for ax in axes:\n",
        "    # Apply a random transformation\n",
        "    augmented_image = datagen.random_transform(x_sample)\n",
        "    ax.imshow(augmented_image)\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# To use data augmentation during training, pass the datagen.flow(...) as the training data in model.fit\n",
        "# Example: model.fit(datagen.flow(X_train, y_train, batch_size=32), ...)"
      ],
      "metadata": {
        "id": "qWkw-w4fMFlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Overview of Different Architectures for Video Analysis\n",
        "\n",
        "Selecting the right architecture for video analysis is pivotal to the success of your machine learning project. Below, we delve into several popular architectures, highlighting their uses and explaining how they work. Understanding these will help you choose the best fit for your project's needs.\n",
        "\n",
        "## CNNs (Convolutional Neural Networks)\n",
        "\n",
        "- **Use:** Primarily for extracting spatial features from video frames.\n",
        "- **Explanation:** CNNs excel in identifying patterns, shapes, and textures within images, making them suitable for frame-level analysis.\n",
        "- **Foundational Papers:** [Gradient-based learning applied to document recognition by LeCun et al.](https://ieeexplore.ieee.org/document/726791)\n",
        "\n",
        "## RNN/LSTM/GRU\n",
        "\n",
        "- **Use:** Best for analyzing temporal dependencies in video sequences.\n",
        "- **Explanation:** RNNs and their variants, LSTM and GRU, are designed to model sequential data, capturing the temporal dynamics crucial for understanding video content over time.\n",
        "- **Foundational Papers:**\n",
        "  - RNN: [Finding Structure in Time by Elman.](https://crl.ucsd.edu/~elman/Papers/fsit.pdf)\n",
        "  - LSTM: [Long Short-Term Memory by Hochreiter & Schmidhuber.](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
        "  - GRU: [Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation by Cho et al.](https://arxiv.org/abs/1406.1078)\n",
        "\n",
        "## 3D CNNs\n",
        "\n",
        "- **Use:** For analyzing videos by extracting both spatial and temporal features.\n",
        "- **Explanation:** By adding a time dimension to the convolutional layers, 3D CNNs can process sequences of frames, making them adept at recognizing actions and events in videos.\n",
        "- **Foundational Paper:** [3D Convolutional Neural Networks for Human Action Recognition by Ji et al.](https://ieeexplore.ieee.org/document/6165309)\n",
        "\n",
        "## Two-Stream Networks\n",
        "\n",
        "- **Use:** For a comprehensive analysis by considering both spatial and temporal information.\n",
        "- **Explanation:** This approach uses a dual-stream model, one for spatial features from single frames and another for temporal features from frame sequences, offering a balanced analysis.\n",
        "- **Foundational Paper:** [Two-stream convolutional networks for action recognition in videos by Simonyan & Zisserman.](https://arxiv.org/abs/1406.2199)\n",
        "\n",
        "\n",
        "## Transformers (e.g., Vision Transformers - ViT)\n",
        "\n",
        "- **Use:** For tasks where capturing long-range dependencies within videos is crucial.\n",
        "- **Explanation:** Adapting the attention mechanism from NLP, Vision Transformers process videos in a manner that emphasizes the interrelation of different parts of the video, both spatially and temporally.\n",
        "- **Foundational Papers:**\n",
        "  - General Transformers: [Attention is All You Need by Vaswani et al.](https://arxiv.org/abs/1706.03762)\n",
        "  - Vision Transformers: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "aCcq0xF_YBZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing Your Neural Architecture Template\n",
        "\n",
        "When selecting an architecture, consider the following:\n",
        "\n",
        "- **Nature of the Task:** Is your focus on understanding the content of individual frames (CNN), the movement between frames (RNN, 3D CNN), or a combination of both (Two-Stream, Transformers)?\n",
        "- **Complexity of the Video Data:** More complex data might benefit from architectures that can capture a wide range of dependencies, like Transformers.\n",
        "- **Computational Resources:** Some models, especially 3D CNNs and Transformers, are more computationally intensive than others.\n",
        "\n",
        "### Tips for Customization:\n",
        "\n",
        "- **Adjusting Parameters:** Tailor parameters like `frame_height`, `frame_width`, and `num_classes` according to your dataset.\n",
        "- **Preprocessing Needs:** Different architectures may require specific forms of input preprocessing. Ensure your data pipeline is compatible with your chosen model.\n",
        "\n",
        "After considering these aspects, select the architecture template that aligns with your project's goals from the list below. Each choice entails specific considerations for dataset dimensions and task specifications (e.g., classification, detection).\n",
        "\n",
        "1. **CNN for Spatial Features:** Ideal for projects focusing on frame-level analysis.\n",
        "2. **RNN/LSTM for Temporal Features:** Suited for understanding sequences and temporal patterns.\n",
        "3. **3D CNN for Spatio-Temporal Features:** Best for capturing actions and events over time.\n",
        "4. **Two-Stream Network:** Offers a comprehensive analysis by leveraging both spatial and temporal data.\n",
        "5. **Transformers:** For advanced projects requiring attention to complex patterns in large datasets.\n",
        "\n",
        "Remember to customize your model based on the specific needs of your dataset and the computational resources available to you."
      ],
      "metadata": {
        "id": "1snlAzZ-cxwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic CNN Model for Spatial Feature Extraction\n",
        "\n",
        "This CNN model is structured to extract spatial features from individual video frames, making it suitable for image-based analysis tasks within videos. Below is the template for creating a basic CNN:"
      ],
      "metadata": {
        "id": "KkaJRq9B8EMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic CNN Model for Spatial Feature Extraction\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Adjust these parameters to fit your dataset\n",
        "frame_height = 112  # Height of the video frame\n",
        "frame_width = 112   # Width of the video frame\n",
        "num_classes = 7    # Number of output classes\n",
        "\n",
        "# Basic CNN Model\n",
        "model_cnn = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(frame_height, frame_width, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "\n",
        "model_cnn.summary()\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-3SSfTJwYPlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN/LSTM for Temporal Feature Extraction\n",
        "\n",
        "This cell demonstrates how to set up a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) layers. It's designed for extracting temporal features from sequences of video frames, which is crucial for understanding activities, actions, or any phenomena that evolve over time."
      ],
      "metadata": {
        "id": "rsu1iTZT7G0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN/LSTM for Temporal Feature Extraction\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Adjust these parameters to fit your dataset\n",
        "timesteps = 100  # Length of your sequences\n",
        "features = 128   # Features extracted from each frame or timestep\n",
        "num_classes = 10 # Number of output classes\n",
        "\n",
        "# RNN Model with LSTM\n",
        "model_rnn = Sequential([\n",
        "    LSTM(64, input_shape=(timesteps, features), return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "\n",
        "model_rnn.summary()\n",
        "\n",
        "# Compile the model\n",
        "model_rnn.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Uc_PvCXraxti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3D CNN for Spatio-Temporal Feature Extraction\n",
        "\n",
        "This section introduces the setup for a 3D Convolutional Neural Network (3D CNN) designed to capture both spatial and temporal features from video clips. This model is capable of understanding motion and changes across consecutive frames, which is essential for tasks like action recognition.\n",
        "\n",
        "# 3D CNN for Spatio-Temporal Feature Extraction\n",
        "\n",
        "A 3D CNN extends the capabilities of traditional CNNs by analyzing sequences of frames to capture temporal dynamics alongside spatial features. Here's how to implement a 3D CNN model:"
      ],
      "metadata": {
        "id": "yKFCU0Os8M7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3D CNN for Spatio-Temporal Feature Extraction\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten\n",
        "\n",
        "# Adjust these parameters to fit your dataset\n",
        "frames_per_clip = 16    # Number of frames per video clip\n",
        "frame_height = 112      # Height of the video frame\n",
        "frame_width = 112       # Width of the video frame\n",
        "num_channels = 3        # Number of color channels (RGB)\n",
        "num_classes = 10        # Number of output classes\n",
        "\n",
        "# 3D CNN Model\n",
        "model_3dcnn = models.Sequential([\n",
        "    Conv3D(64, (3, 3, 3), activation='relu',\n",
        "           input_shape=(frames_per_clip, frame_height, frame_width, num_channels)),\n",
        "    MaxPooling3D((2, 2, 2)),\n",
        "    Conv3D(128, (3, 3, 3), activation='relu'),\n",
        "    MaxPooling3D((2, 2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax'),\n",
        "])\n",
        "\n",
        "model_3dcnn.summary()\n",
        "\n",
        "# Compile the model\n",
        "model_3dcnn.compile(optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "WUSdQxatbGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Two-Stream Networks for Video Analysis\n",
        "\n",
        "Implementing a two-stream network involves creating and training two separate models: one focused on spatial features (using a CNN) and another on temporal features (using either a 3D CNN or an RNN). The predictions from these models are then combined to produce a final output. This combination can be achieved through simple averaging or a more complex learned fusion layer.\n",
        "\n",
        "**Note:** This approach is conceptual, and specific implementation details will vary based on your project's needs.\n",
        "\n",
        "### Implementation Overview:\n",
        "\n",
        "- **For Spatial Features:** Utilize the CNN model outlined earlier.\n",
        "- **For Temporal Features:** Employ either the 3D CNN model or an RNN model, depending on the nature of your data and the specific temporal dynamics you wish to capture.\n",
        "- **Combining Models:** A straightforward method to combine these models is to average their predictions:"
      ],
      "metadata": {
        "id": "UCFBG3Xu4GwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of combining predictions\n",
        "predictions = 0.5 * cnn_model.predict(spatial_data) + 0.5 * temporal_model.predict(temporal_data)"
      ],
      "metadata": {
        "id": "0AZVrSh4bOdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vision Transformers for Video Processing\n",
        "\n",
        "Transformers have revolutionized natural language processing and are now making significant inroads into computer vision, including video processing tasks. Vision Transformers (ViT) apply the transformer architecture to image patches, treating each patch as a token similar to how words are treated in NLP. This method allows for capturing complex spatial hierarchies and has been extended to video processing to handle temporal dynamics as well.\n",
        "\n",
        "# Transformers for Video Processing (Vision Transformers - ViT)\n",
        "\n",
        "Vision Transformers (ViT) represent a novel approach in leveraging transformer architectures for video processing tasks. By decomposing video frames into a sequence of patches and processing these patches as tokens, ViTs can capture intricate spatial-temporal relationships within the video content.\n",
        "\n",
        "### Preliminary Steps:\n",
        "\n",
        "- **Base Model for Feature Extraction:** Leveraging pre-trained models like EfficientNet as a starting point for extracting features from video frames can be beneficial. These features then serve as inputs to the transformer model."
      ],
      "metadata": {
        "id": "3xxpEyRR6AMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers for Video Processing (Vision Transformers - ViT)\n",
        "\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from vit_keras import vit\n",
        "\n",
        "# Example for loading a pre-trained model\n",
        "base_model = EfficientNetB0(include_top=False, weights='imagenet')"
      ],
      "metadata": {
        "id": "4ToSvDVLbRUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Training and Monitoring the Model\n",
        "\n",
        "Training your machine learning model is a crucial step where the model learns to recognize patterns from the data. To track the model's progress and ensure that we save the best version, we'll employ callbacks like `ModelCheckpoint` and `TensorBoard`.\n"
      ],
      "metadata": {
        "id": "wTVzLBdDlt_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "import numpy as np\n",
        "\n",
        "# Loading the dataset splits\n",
        "data = np.load('/content/dataset_splits.npz') # Adjust according to your dataset path\n",
        "X_train = data['X_train']\n",
        "X_val = data['X_val']\n",
        "y_train = data['y_train']\n",
        "y_val = data['y_val']\n",
        "\n",
        "# Setup callbacks\n",
        "checkpoint_cb = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "tensorboard_cb = TensorBoard(log_dir='./logs')\n",
        "\n",
        "# Train the model\n",
        "history = model_cnn.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=12,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[checkpoint_cb, tensorboard_cb]\n",
        ")"
      ],
      "metadata": {
        "id": "dbPToovalwOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enabling TensorBoard within the notebook environment\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./logs"
      ],
      "metadata": {
        "id": "FFUXSfwo7My5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Hyperparameter Tuning\n",
        "\n",
        "Hyperparameter tuning is a pivotal phase in the machine learning pipeline. This process involves optimizing the model's architecture and the training procedure to maximize its performance. Hyperparameters, unlike the model's internal parameters learned during training, need to be set beforehand and have a substantial impact on the model's learning efficiency and output quality.\n",
        "\n",
        "Commonly tuned hyperparameters include:\n",
        "- **Learning Rate**: Controls how much to adjust the model in response to the estimated error each time the model weights are updated.\n",
        "- **Batch Size**: Number of training examples utilized in one iteration.\n",
        "- **Number of Epochs**: Total number of times the training dataset is passed forward and backward through the neural network.\n",
        "- **Architecture-Specific Parameters**: Such as the number of layers or units in a layer, which can vary significantly across different models.\n",
        "\n",
        "### Approaches to Hyperparameter Tuning:\n",
        "\n",
        "1. **Manual Tuning**: Relying on experience and intuition to adjust hyperparameters.\n",
        "2. **Grid Search**: Exhaustively searching through a predefined list of hyperparameter values.\n",
        "3. **Random Search**: Randomly selecting hyperparameter values from a defined range and evaluating their performance.\n",
        "4. **Bayesian Optimization**: Using probabilistic models to guide the search for the optimum hyperparameters.\n",
        "5. **Automated Tools**: Leveraging tools like Keras Tuner or Hyperopt to automate the tuning process.\n",
        "\n",
        "### Implementing Hyperparameter Tuning:\n",
        "\n",
        "For practical hyperparameter tuning, consider starting with either Grid Search or Random Search as they are straightforward to implement and can yield significant improvements:\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "mfOP6GAlS7aW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual Tuning Approach\n",
        "\n",
        "1. **Start with a baseline model**: Set up your model with a default set of hyperparameters.\n",
        "2. **Identify key hyperparameters**: Focus on those most likely to impact performance, such as learning rate or the number of layers.\n",
        "3. **Iteratively adjust values**: Manually change one hyperparameter at a time and monitor the effect on model performance.\n",
        "4. **Use a systematic approach**: Keep a log of changes and results to guide future adjustments.\n",
        "5. **Refinement**: Once the model shows improvement, refine your search around the best-performing values."
      ],
      "metadata": {
        "id": "wlCsjVKiAQah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grid Search Template\n",
        "\n",
        "Grid Search exhaustively tests a predefined range of hyperparameter values, ensuring that you explore all possible combinations within your specified grid. This method is particularly useful when the number of hyperparameters and their potential values are relatively low."
      ],
      "metadata": {
        "id": "hd3YJqPaFre4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "# Assume 'build_model' is a function that constructs a Keras model\n",
        "def build_model(optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_shape=(input_dim,), activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=build_model)\n",
        "\n",
        "param_grid = {\n",
        "    'epochs': [10, 20],\n",
        "    'batch_size': [16, 32],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Access the best set of parameters\n",
        "best_params = grid_result.best_params_"
      ],
      "metadata": {
        "id": "NUnBYKDEAjea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Search Template\n",
        "\n",
        "Random Search optimizes hyperparameters by sampling values from a defined distribution. This method can be more efficient than Grid Search, particularly when the hyperparameter space is large."
      ],
      "metadata": {
        "id": "nLjTu-32F_Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "def build_model(optimizer='adam'):\n",
        "    # Model construction (omitted for brevity)\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=build_model, epochs=20, batch_size=32)\n",
        "\n",
        "param_dist = {\n",
        "    'batch_size': [16, 32, 64],\n",
        "    'epochs': [10, 20, 30],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, n_jobs=-1, cv=3)\n",
        "random_search_result = random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "best_params = random_search_result.best_params_"
      ],
      "metadata": {
        "id": "35b3pOd3FMtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bayesian Optimization Template\n",
        "\n",
        "Bayesian Optimization leverages a probabilistic model to select hyperparameters that are likely to yield better results. This method is efficient for finding optimal hyperparameters with fewer trials, making it ideal for complex models."
      ],
      "metadata": {
        "id": "BqrmAtNxGMPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "def objective(space):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=int(space['units']), input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(learning_rate=space['learning_rate']),\n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=int(space['batch_size']), verbose=0)\n",
        "\n",
        "    val_loss = np.min(history.history['val_loss'])\n",
        "    return {'loss': val_loss, 'status': STATUS_OK}\n",
        "\n",
        "space = {\n",
        "    'units': hp.quniform('units', 50, 150, 1),\n",
        "    'batch_size': hp.choice('batch_size', [16, 32, 64]),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
        "}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(fn=objective,\n",
        "            space=space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=100,\n",
        "            trials=trials)\n",
        "\n",
        "best_params = space_eval(space, best)"
      ],
      "metadata": {
        "id": "MaBKp3nZFSUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning with Keras Tuner\n",
        "\n",
        "Keras Tuner simplifies the task of finding the best hyperparameters for your model. The example below outlines how to use Keras Tuner to optimize a Convolutional Neural Network (CNN) architecture."
      ],
      "metadata": {
        "id": "9L7zyrb--run"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_tuner import RandomSearch\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "\n",
        "input_shape = (112, 112, 3)  # Example input shape; adjust as necessary\n",
        "num_classes = 10  # Adjust based on your dataset\n",
        "\n",
        "# Function to build the model (required for Keras Tuner)\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters=hp.Int('conv_filters', min_value=32, max_value=128, step=32),\n",
        "                     kernel_size=hp.Choice('conv_kernel_size', values=[3, 5]),\n",
        "                     activation='relu',\n",
        "                     input_shape=input_shape))\n",
        "    model.add(MaxPooling2D())\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=128, step=32),\n",
        "                    activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='hparam_tuning'\n",
        ")\n",
        "\n",
        "# Perform the search\n",
        "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Retrieve the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "print(f\"\"\"\n",
        "The optimal number of units in the first dense layer is {best_hps.get('dense_units')} and the\n",
        "optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n",
        "\n",
        "# Rebuild the model with the best hyperparameters and train it\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "IAj1IU7JYyga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Evaluating the Model\n",
        "\n",
        "After training, it's crucial to evaluate your model on the test set to understand its performance on unseen data. This step gives you insights into how well your model has learned and generalized from the training data.\n"
      ],
      "metadata": {
        "id": "uFSCmiR4mAoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "test_loss, test_acc = model_cnn.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc:.2f}\")"
      ],
      "metadata": {
        "id": "jq40-ytbmBSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Regularization Techniques\n",
        "\n",
        "Regularization techniques are critical in preventing overfitting, especially when you have a high-capacity model or limited data. Overfitting occurs when a model learns the noise in the training data to the extent that it negatively impacts the performance of the model on new data. Regularization methods provide ways to penalize model complexity or introduce noise to the training process to promote the generalizability of the model.\n",
        "\n",
        "There are several regularization techniques:\n",
        "- **L1 and L2 Regularization**: Penalizes the weights of the model during training, which can help to prevent overfitting by encouraging simpler models that may generalize better.\n",
        "- **Dropout**: Randomly sets a fraction of input units to 0 at each update during training time, which helps to prevent overfitting by making the neural network less sensitive to the specific weights of neurons.\n",
        "- **Batch Normalization**: Although primarily used to normalize the input layer by adjusting and scaling the activations, it can also have a regularizing effect.\n",
        "- **Early Stopping**: Monitors the model's performance on a validation set and stops training when performance begins to degrade.\n",
        "\n",
        "In the following code cells, we will add L2 regularization and dropout to our neural network to help with overfitting. We'll also implement early stopping to halt the training process at the optimal moment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QpMbGniVTMOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Modify your model definition to include L2 regularization and dropout\n",
        "def build_regularized_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(frame_height, frame_width, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)),  # L2 Regularization\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.5),  # Dropout layer\n",
        "        layers.Dense(num_classes, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Instantiate the regularized model\n",
        "model_regularized = build_regularized_model()\n",
        "\n",
        "# Include an EarlyStopping callback\n",
        "early_stopping_cb = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history_regularized = model_regularized.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,  # Set a high epoch limit; training may stop early\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[checkpoint_cb, tensorboard_cb, early_stopping_cb]  # Add early stopping to your list of callbacks\n",
        ")\n",
        "\n",
        "# Evaluate the regularized model on the test set\n",
        "test_loss_reg, test_acc_reg = model_regularized.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy with regularization: {test_acc_reg:.2f}\")"
      ],
      "metadata": {
        "id": "PLIDKt9ZZXdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing for Fine-Tuning (if needed)\n",
        "\n",
        "To fine-tune your model, identify which layers need adjustment and set a lower learning rate for fine-tuning. This approach delicately refines the model's ability to adapt to your specific dataset."
      ],
      "metadata": {
        "id": "n9YozC24nyUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example for a model with a pre-trained base\n",
        "for layer in model.layers[:layer_to_freeze]:\n",
        "    layer.trainable = False  # Freeze layers not intended for fine-tuning\n",
        "for layer in model.layers[layer_to_freeze:]:\n",
        "    layer.trainable = True  # Unfreeze layers for fine-tuning\n",
        "\n",
        "# Adjust the learning rate for fine-tuning\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)  # Lower learning rate\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summarize the model post adjustments to verify changes\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "lGkv5IT1ny87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-Tuning the Model With the model adjusted for fine-tuning,\n",
        "# continue training to refine its performance on the dataset. Monitor the training process closely to ensure improvements.\n",
        "\n",
        "# Continue training the model\n",
        "history_fine = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=10,  # Adjust epochs based on when performance plateaus\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[checkpoint_cb, tensorboard_cb]  # Reuse callbacks from initial training\n",
        ")"
      ],
      "metadata": {
        "id": "pnhidTZwoB4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model Post Fine-Tuning\n",
        "\n",
        "After fine-tuning, evaluate the model again on the test set to assess any improvements. This step helps understand the effectiveness of your fine-tuning efforts.\n"
      ],
      "metadata": {
        "id": "MreWcG6_ol7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model again\n",
        "test_loss, test_acc = model_cnn.evaluate(X_test, y_test)\n",
        "print(f\"Post Fine-Tuning Test Accuracy: {test_acc:.2f}\")"
      ],
      "metadata": {
        "id": "DfzE2yYZonpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Trained Model\n",
        "\n",
        "After training and fine-tuning, save your model to reuse it later without needing to retrain. This step is crucial for deployment."
      ],
      "metadata": {
        "id": "5CfDUZjtq3fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Save the entire model to an H5 file\n",
        "model_cnn.save('my_model.h5')\n",
        "\n",
        "# Check if the model file exists in the current directory\n",
        "if os.path.exists('my_model.h5'):\n",
        "    print('Model saved successfully as my_model.h5')\n",
        "else:\n",
        "    print('Model saving failed.')"
      ],
      "metadata": {
        "id": "519WwJCvq4Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading and Inference"
      ],
      "metadata": {
        "id": "cYRh1nzRTlgW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CF5DeERgT3bO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}